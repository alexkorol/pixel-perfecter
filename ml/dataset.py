"""
Dataset utilities for ML-assisted grid estimation.

Reads manifest entries generated by ``ml.prepare_dataset`` and produces
torch-compatible datasets that expose RGB + edge channel tensors together with
normalised grid parameters.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Sequence

import cv2
import numpy as np
import torch
from PIL import Image, ImageOps
from torchvision import transforms as T
from torch.utils.data import Dataset


MANIFEST_FILENAME = "manifest.jsonl"
MAX_NORMALISATION = 256.0  # clamp for cell size / offsets


@dataclass
class ManifestEntry:
    """Represents a single labelled sample."""

    path: Path
    cell_size: float
    offset_x: float
    offset_y: float
    label_source: str
    weight: float
    split: str


def load_manifest(manifest_path: Path) -> List[ManifestEntry]:
    """Load manifest entries written by ``prepare_dataset``."""
    entries: List[ManifestEntry] = []
    if not manifest_path.exists():
        raise FileNotFoundError(f"Manifest not found: {manifest_path}")

    with manifest_path.open("r", encoding="utf-8") as fh:
        for line in fh:
            record = json.loads(line)
            entries.append(
                ManifestEntry(
                    path=Path(record["image_path"]),
                    cell_size=float(record["cell_size"]),
                    offset_x=float(record["offset_x"]),
                    offset_y=float(record["offset_y"]),
                    label_source=record["label_source"],
                    weight=float(record.get("weight", 1.0)),
                    split=record["split"],
                )
            )
    return entries


class GridDataset(Dataset):
    """
    Torch dataset returning stacked RGB+edge channel tensors.

    Each item is ``(tensor, target, sample_weight)`` where:
        * tensor: FloatTensor with shape (4, H, W) normalised to [0, 1]
        * target: FloatTensor of shape (3,) with normalised parameters
        * sample_weight: FloatTensor scalar reflecting label confidence
    """

    def __init__(
        self,
        entries: Sequence[ManifestEntry],
        image_size: int = 160,
        augment: bool = False,
    ) -> None:
        self.entries = list(entries)
        self.image_size = image_size
        self.augment = augment
        self._augment_transform = None
        if self.augment:
            self._augment_transform = T.Compose(
                [
                    T.RandomApply(
                        [T.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2)],
                        p=0.6,
                    ),
                    T.RandomAutocontrast(p=0.3),
                    T.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),
                    T.RandomApply(
                        [T.GaussianBlur(kernel_size=3, sigma=(0.1, 0.7))], p=0.3
                    ),
                ]
            )

    def __len__(self) -> int:
        return len(self.entries)

    def __getitem__(self, idx: int):
        entry = self.entries[idx]
        pil_image, scale = self._load_rgb(entry.path)
        if self.augment and self._augment_transform is not None:
            pil_image = self._augment_transform(pil_image)

        image = np.array(pil_image, dtype=np.uint8)
        if self.augment:
            image = self._inject_noise(image)

        edges = self._compute_edges(image)

        tensor = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
        edge_tensor = torch.from_numpy(edges).unsqueeze(0).float()
        stacked = torch.cat([tensor, edge_tensor], dim=0)

        scale = float(scale)
        scaled_cell = entry.cell_size * scale
        scaled_off_x = entry.offset_x * scale
        scaled_off_y = entry.offset_y * scale

        target = torch.tensor(
            [
                min(scaled_cell, MAX_NORMALISATION) / MAX_NORMALISATION,
                min(scaled_off_x, MAX_NORMALISATION) / MAX_NORMALISATION,
                min(scaled_off_y, MAX_NORMALISATION) / MAX_NORMALISATION,
            ],
            dtype=torch.float32,
        )

        weight = torch.tensor(entry.weight, dtype=torch.float32)
        scale_tensor = torch.tensor(scale, dtype=torch.float32)
        return stacked, target, weight, scale_tensor

    def _load_rgb(self, path: Path) -> tuple[Image.Image, float]:
        if not path.exists():
            raise FileNotFoundError(f"Image not found: {path}")
        with Image.open(path) as img:
            img = img.convert("RGB")
            orig_w, orig_h = img.size
            scale = self.image_size / max(orig_w, orig_h)
            img = ImageOps.pad(
                img,
                (self.image_size, self.image_size),
                method=Image.BILINEAR,
                color=(0, 0, 0),
            )
            return img.copy(), scale

    def _compute_edges(self, image: np.ndarray) -> np.ndarray:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        return edges.astype(np.float32) / 255.0

    def _inject_noise(self, image: np.ndarray) -> np.ndarray:
        noise_std = float(np.random.uniform(0.0, 6.0))
        if noise_std < 0.5:
            return image
        noise = np.random.normal(0.0, noise_std, image.shape).astype(np.float32)
        noisy = np.clip(image.astype(np.float32) + noise, 0.0, 255.0)
        return noisy.astype(np.uint8)


def split_entries(
    entries: Iterable[ManifestEntry], val_ratio: float, seed: int
) -> tuple[list[ManifestEntry], list[ManifestEntry]]:
    """Deterministically split entries into train/val lists."""
    rng = np.random.default_rng(seed)
    shuffled = list(entries)
    rng.shuffle(shuffled)

    if not shuffled:
        return [], []

    val_count = max(1, int(len(shuffled) * val_ratio))
    if val_count >= len(shuffled):
        val_count = max(1, len(shuffled) // 5 or 1)

    val_entries = shuffled[:val_count]
    train_entries = shuffled[val_count:]
    if not train_entries:
        train_entries = val_entries
    return train_entries, val_entries
